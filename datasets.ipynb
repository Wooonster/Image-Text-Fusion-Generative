{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset  # ds = load_dataset(\"UCSC-VLAA/MedTrinity-25M\", \"25M_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>8031efe0-1b5c-11ef-8929-000066532cad</td>\n",
       "      <td>The image is a non-contrasted computed tomogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>8031fb83-1b5c-11ef-a2c7-000066532cad</td>\n",
       "      <td>The image is a non-contrast computed tomograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>8032083e-1b5c-11ef-bcf7-000066532cad</td>\n",
       "      <td>The image is a CT scan of the brain, showing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>8031ea0d-1b5c-11ef-b7fd-000066532cad</td>\n",
       "      <td>The image is a non-contrasted computed tomogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>8031f5b4-1b5c-11ef-9ae7-000066532cad</td>\n",
       "      <td>The image is a non-contrasted computed tomogra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  \\\n",
       "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
       "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
       "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
       "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
       "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
       "\n",
       "                                     id  \\\n",
       "0  8031efe0-1b5c-11ef-8929-000066532cad   \n",
       "1  8031fb83-1b5c-11ef-a2c7-000066532cad   \n",
       "2  8032083e-1b5c-11ef-bcf7-000066532cad   \n",
       "3  8031ea0d-1b5c-11ef-b7fd-000066532cad   \n",
       "4  8031f5b4-1b5c-11ef-9ae7-000066532cad   \n",
       "\n",
       "                                             caption  \n",
       "0  The image is a non-contrasted computed tomogra...  \n",
       "1  The image is a non-contrast computed tomograph...  \n",
       "2  The image is a CT scan of the brain, showing t...  \n",
       "3  The image is a non-contrasted computed tomogra...  \n",
       "4  The image is a non-contrasted computed tomogra...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq = pd.read_parquet('./train-00000-of-00010.parquet')\n",
    "pq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) The image is a non-contrasted computed tomography (CT) scan of the brain, showing the cranial cavity with brain structures. The region of interest, located centrally at the top of the image, occupies approximately 0.6% of the area and appears to have a different density compared to the surrounding brain tissue, which may indicate an abnormality such as a hemorrhage or a mass effect. This region's relative position to other brain structures suggests it could be affecting or be affected by adjacent tissues, potentially indicating a pathological process that may have implications for the patient's neurological function.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_data = self.dataframe.iloc[idx]['image']['bytes']\n",
    "        caption = self.dataframe.iloc[idx]['caption']\n",
    "        image = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "        # Check if image_array contains valid data\n",
    "        if np.array(image).sum() == 0:  # 检查像素总和是否为 0\n",
    "            raise ValueError(f\"Image array at index {idx} is all zeros.\")\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        return image, caption\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageCaptionDataset(pq)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for images, captions in dataloader:\n",
    "    print(images[0].shape, captions[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "DEVICE = 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class ImageFeatureExtract(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # resnet = models.resnet50(pretrained=True)  # load pretrained ResNet50 from torchvision\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)  # load pretrained ResNet50 from torchvision\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            *list(\n",
    "                resnet.children()  # iterator over the layers (modules) of the ResNet-50 architecture\n",
    "                )[:-1]  # removes the last layer, the MLP \n",
    "            )  # chain the remaining ResNet-50 layers together\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 2048, 1, 1) \n",
    "        # ResNet-50's final convolutional output is globally averaged to a single spatial position per channel\n",
    "        x = self.feature_extract(x)\n",
    "        return x.view(x.size(0), -1).to(DEVICE)  # flatten the tensor to (batch_size, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape: torch.Size([16, 3, 224, 224])\n",
      "img_features.shape: torch.Size([16, 2048])\n"
     ]
    }
   ],
   "source": [
    "imgs, captions = dataloader.__iter__().__next__()\n",
    "print(f'imgs.shape: {imgs.shape}')\n",
    "\n",
    "img_ext = ImageFeatureExtract().to(DEVICE)\n",
    "imgs = imgs.to(DEVICE)\n",
    "img_features = img_ext(imgs)  # Extract features\n",
    "print(f'img_features.shape: {img_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 16\n",
      "txt_features.shape: torch.Size([16, 176, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Extract text features\n",
    "# use BERT-base\n",
    "class TextFeatureExtract(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        outputs = self.bert(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        return last_hidden_state\n",
    "\n",
    "print(type(captions), len(captions))  # 确保是 List[str]\n",
    "\n",
    "txt_ext = TextFeatureExtract().to(DEVICE)\n",
    "txt_features = txt_ext(captions)\n",
    "print(f'txt_features.shape: {txt_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_map.shape: torch.Size([16, 1024]), txt_map.shape: torch.Size([16, 176, 1024]), fuse_embed.shape: torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Contrastive Learning for mapping text with image\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.map(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "\n",
    "    \n",
    "# img_features.shape: torch.Size([16, 2048])\n",
    "img_mapping = MappingNetwork(2048, 1024).to(DEVICE)\n",
    "# txt_features.shape: torch.Size([16, 180, 768])\n",
    "txt_mapping = MappingNetwork(768, 1024).to(DEVICE)\n",
    "\n",
    "img_map = img_mapping(img_features)  # img_map.shape: torch.Size([16, 1024])\n",
    "txt_map = txt_mapping(txt_features)  # txt_map.shape: torch.Size([16, 180, 1024])\n",
    "fuse_embed = img_map + txt_map.mean(dim=1)  # fuse_embed.shape: torch.Size([16, 1024])\n",
    "print(f'img_map.shape: {img_map.shape}, txt_map.shape: {txt_map.shape}, fuse_embed.shape: {fuse_embed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape: torch.Size([16, 3, 224, 224])\n",
      "Output shape: torch.Size([16, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_data = self.dataframe.iloc[idx]['image']['bytes']\n",
    "        caption = self.dataframe.iloc[idx]['caption']\n",
    "        image = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "        # 检查图像数据是否有效\n",
    "        if np.array(image).sum() == 0:\n",
    "            raise ValueError(f\"Image array at index {idx} is all zeros.\")\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        return image, caption\n",
    "\n",
    "class ImageFeatureExtract(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            *list(resnet.children())[:-1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        return x.view(x.size(0), -1).to(DEVICE)\n",
    "\n",
    "class TextFeatureExtract(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        outputs = self.bert(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        return last_hidden_state\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.map(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # L2 归一化\n",
    "\n",
    "class TwoPathEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义卷积层\n",
    "        self.cnn_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            ) for in_channels, out_channels in zip([1024, 32, 64, 128, 256], [32, 64, 128, 256, 512])\n",
    "        ])\n",
    "        \n",
    "        # 定义用于调整 pooled 通道数的 1x1 卷积层\n",
    "        self.pool_conv_layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            for in_channels, out_channels in zip([1024, 32, 64, 128, 256], [32, 64, 128, 256, 512])\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = x\n",
    "        for layer, pool_conv in zip(self.cnn_layers, self.pool_conv_layers):\n",
    "            cnn_out = layer(x)  # 输出形状 [batch, out_channels, h, w]\n",
    "            # 使用自适应池化，使 pooled 的空间尺寸与 cnn_out 相同\n",
    "            pooled = F.adaptive_max_pool2d(pooled, output_size=cnn_out.shape[2:])\n",
    "            pooled = pool_conv(pooled)  # 调整通道数，输出形状 [batch, out_channels, h, w]\n",
    "            x = cnn_out + pooled  # 相加，确保形状匹配\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoded_dim=512, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1024, 7 * 7 * encoded_dim)  # 将融合向量映射到空间表示\n",
    "        self.deconv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            ) for in_channels, out_channels in zip([512, 256, 128, 64, 32], [256, 128, 64, 32, 16])\n",
    "        ])\n",
    "        self.last_conv = nn.Conv2d(16, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, z, encoded):\n",
    "        z = self.fc(z).view(-1, 512, 7, 7)\n",
    "        x = z + encoded\n",
    "        for layer in self.deconv_layers:\n",
    "            x = layer(x)\n",
    "        return self.last_conv(x)\n",
    "\n",
    "class TextImageFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_extract = ImageFeatureExtract().to(DEVICE)\n",
    "        self.txt_extract = TextFeatureExtract().to(DEVICE)\n",
    "        self.img_map = MappingNetwork(2048, 1024).to(DEVICE)\n",
    "        self.txt_map = MappingNetwork(768, 1024).to(DEVICE)\n",
    "        self.fusion_to_spatial = nn.Linear(1024, 1024 * 7 * 7).to(DEVICE)  # 新增线性层\n",
    "        self.encoder = TwoPathEncoder().to(DEVICE)\n",
    "        self.decoder = Decoder().to(DEVICE)\n",
    "\n",
    "    def forward(self, image, text_ids):\n",
    "        # Step 1: 提取特征\n",
    "        img_fea = self.img_extract(image)  # [batch_size, 2048]\n",
    "        txt_fea = self.txt_extract(text_ids)  # [batch_size, seq_length, 768]\n",
    "        txt_fea = txt_fea.mean(dim=1)  # 池化: [batch_size, 768]\n",
    "\n",
    "        # Step 2: 映射特征\n",
    "        img_mapped = self.img_map(img_fea)  # [batch_size, 1024]\n",
    "        txt_mapped = self.txt_map(txt_fea)  # [batch_size, 1024]\n",
    "\n",
    "        # Step 3: 融合特征\n",
    "        fused = img_mapped + txt_mapped  # [batch_size, 1024]\n",
    "\n",
    "        # Step 4: 映射到空间维度\n",
    "        fused_spatial = self.fusion_to_spatial(fused)  # [batch_size, 1024*7*7]\n",
    "        fused_spatial = fused_spatial.view(fused.size(0), 1024, 7, 7)  # [batch_size, 1024, 7, 7]\n",
    "\n",
    "        # Step 5: 编码和解码\n",
    "        encoded = self.encoder(fused_spatial)  # [batch_size, 512, h, w]\n",
    "        output = self.decoder(fused, encoded)  # [batch_size, output_channels, h, w]\n",
    "\n",
    "        return output\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "# 假设 pq 是已经定义的 dataframe\n",
    "dataset = ImageCaptionDataset(pq)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 创建模型并移动到设备\n",
    "model = TextImageFusionModel().to(DEVICE)\n",
    "\n",
    "# 获取一个批次的数据\n",
    "imgs, captions = next(iter(dataloader))\n",
    "print(f'imgs.shape: {imgs.shape}')  # 应为 [16, 3, 224, 224]\n",
    "\n",
    "# 传递给模型\n",
    "output = model(imgs, captions)\n",
    "print(f\"Output shape: {output.shape}\")  # 预期形状，例如 [16, 1, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
